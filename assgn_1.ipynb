{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import idx2numpy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from random import randint\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "file = './train-images.idx3-ubyte'\n",
    "x_train = np.reshape(idx2numpy.convert_from_file(file),(60000,784))\n",
    "file = './train-labels.idx1-ubyte'\n",
    "y_train = np.reshape(idx2numpy.convert_from_file(file),(60000))\n",
    "file = './t10k-images.idx3-ubyte'\n",
    "x_test = np.reshape(idx2numpy.convert_from_file(file),(10000,784))\n",
    "file = './t10k-labels.idx1-ubyte'\n",
    "y_test = np.reshape(idx2numpy.convert_from_file(file),10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 11:11:34.007914 15152 meta_graph.py:449] Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'X' has type <class 'str'>, but expected one of: (<class 'bool'>, <class 'numbers.Integral'>)\n",
      "W0928 11:11:34.129759 15152 meta_graph.py:449] Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'X' has type <class 'str'>, but expected one of: (<class 'bool'>, <class 'numbers.Integral'>)\n"
     ]
    }
   ],
   "source": [
    "k = 15\n",
    "points           = tf.Variable(x_train, 'X', dtype=tf.float32)\n",
    "ones_like        = tf.ones((points.get_shape()[0], 1))\n",
    "prev_assignments = tf.Variable(tf.zeros((points.get_shape()[0], ), dtype=tf.int64))\n",
    "\n",
    "# select random points as a starting position. You can do better by randomly selecting k points.\n",
    "start_pos = tf.Variable(x_train[np.random.randint(60000, size=k),:], dtype=tf.float32)\n",
    "centroids = tf.Variable(start_pos.initialized_value(), 'S', dtype=tf.float32)\n",
    "\n",
    "# find the distance between all points: http://stackoverflow.com/a/43839605/1090562\n",
    "p1 = tf.matmul(\n",
    "    tf.expand_dims(tf.reduce_sum(tf.square(points), 1), 1),\n",
    "    tf.ones(shape=(1, k))\n",
    ")\n",
    "p2 = tf.transpose(tf.matmul(\n",
    "    tf.reshape(tf.reduce_sum(tf.square(centroids), 1), shape=[-1, 1]),\n",
    "    ones_like,\n",
    "    transpose_b=True\n",
    "))\n",
    "distance = tf.sqrt(tf.add(p1, p2) - 2 * tf.matmul(points, centroids, transpose_b=True))\n",
    "\n",
    "# assign each point to a closest centroid\n",
    "point_to_centroid_assignment = tf.argmin(distance, axis=1)\n",
    "\n",
    "# recalculate the centers\n",
    "total = tf.unsorted_segment_sum(points, point_to_centroid_assignment, k)\n",
    "count = tf.unsorted_segment_sum(ones_like, point_to_centroid_assignment, k)\n",
    "means = total / count\n",
    "\n",
    "# continue if there is any difference between the current and previous assignment\n",
    "is_continue = tf.reduce_any(tf.not_equal(point_to_centroid_assignment, prev_assignments))\n",
    "\n",
    "with tf.control_dependencies([is_continue]):\n",
    "    loop = tf.group(centroids.assign(means), prev_assignments.assign(point_to_centroid_assignment))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./log\",sess.graph)\n",
    "\n",
    "\n",
    "# do many iterations\n",
    "has_changed, cnt = True, 0\n",
    "while has_changed and cnt < 300:\n",
    "    cnt += 1\n",
    "    has_changed, _ = sess.run([is_continue, loop])\n",
    "\n",
    "# see how the data is assigned\n",
    "res1 = sess.run(point_to_centroid_assignment)\n",
    "means1 = sess.run(means)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 20\n",
    "points           = tf.Variable(x_train, 'X', dtype=tf.float32)\n",
    "ones_like        = tf.ones((points.get_shape()[0], 1))\n",
    "prev_assignments = tf.Variable(tf.zeros((points.get_shape()[0], ), dtype=tf.int64))\n",
    "\n",
    "# select random points as a starting position. You can do better by randomly selecting k points.\n",
    "start_pos = tf.Variable(x_train[np.random.randint(60000, size=k),:], dtype=tf.float32)\n",
    "centroids = tf.Variable(start_pos.initialized_value(), 'S', dtype=tf.float32)\n",
    "\n",
    "# find the distance between all points: http://stackoverflow.com/a/43839605/1090562\n",
    "p1 = tf.matmul(\n",
    "    tf.expand_dims(tf.reduce_sum(tf.square(points), 1), 1),\n",
    "    tf.ones(shape=(1, k))\n",
    ")\n",
    "p2 = tf.transpose(tf.matmul(\n",
    "    tf.reshape(tf.reduce_sum(tf.square(centroids), 1), shape=[-1, 1]),\n",
    "    ones_like,\n",
    "    transpose_b=True\n",
    "))\n",
    "distance = tf.sqrt(tf.add(p1, p2) - 2 * tf.matmul(points, centroids, transpose_b=True))\n",
    "\n",
    "# assign each point to a closest centroid\n",
    "point_to_centroid_assignment = tf.argmin(distance, axis=1)\n",
    "\n",
    "# recalculate the centers\n",
    "total = tf.unsorted_segment_sum(points, point_to_centroid_assignment, k)\n",
    "count = tf.unsorted_segment_sum(ones_like, point_to_centroid_assignment, k)\n",
    "means = total / count\n",
    "\n",
    "# continue if there is any difference between the current and previous assignment\n",
    "is_continue = tf.reduce_any(tf.not_equal(point_to_centroid_assignment, prev_assignments))\n",
    "\n",
    "with tf.control_dependencies([is_continue]):\n",
    "    loop = tf.group(centroids.assign(means), prev_assignments.assign(point_to_centroid_assignment))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# do many iterations\n",
    "has_changed, cnt = True, 0\n",
    "while has_changed and cnt < 300:\n",
    "    cnt += 1\n",
    "    has_changed, _ = sess.run([is_continue, loop])\n",
    "\n",
    "# see how the data is assigned\n",
    "res2 = sess.run(point_to_centroid_assignment)\n",
    "means2 = sess.run(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25\n",
    "points           = tf.Variable(x_train, 'X', dtype=tf.float32)\n",
    "ones_like        = tf.ones((points.get_shape()[0], 1))\n",
    "prev_assignments = tf.Variable(tf.zeros((points.get_shape()[0], ), dtype=tf.int64))\n",
    "\n",
    "# select random points as a starting position. You can do better by randomly selecting k points.\n",
    "start_pos = tf.Variable(x_train[np.random.randint(60000, size=k),:], dtype=tf.float32)\n",
    "centroids = tf.Variable(start_pos.initialized_value(), 'S', dtype=tf.float32)\n",
    "\n",
    "# find the distance between all points: http://stackoverflow.com/a/43839605/1090562\n",
    "p1 = tf.matmul(\n",
    "    tf.expand_dims(tf.reduce_sum(tf.square(points), 1), 1),\n",
    "    tf.ones(shape=(1, k))\n",
    ")\n",
    "p2 = tf.transpose(tf.matmul(\n",
    "    tf.reshape(tf.reduce_sum(tf.square(centroids), 1), shape=[-1, 1]),\n",
    "    ones_like,\n",
    "    transpose_b=True\n",
    "))\n",
    "distance = tf.sqrt(tf.add(p1, p2) - 2 * tf.matmul(points, centroids, transpose_b=True))\n",
    "\n",
    "# assign each point to a closest centroid\n",
    "point_to_centroid_assignment = tf.argmin(distance, axis=1)\n",
    "\n",
    "# recalculate the centers\n",
    "total = tf.unsorted_segment_sum(points, point_to_centroid_assignment, k)\n",
    "count = tf.unsorted_segment_sum(ones_like, point_to_centroid_assignment, k)\n",
    "means = total / count\n",
    "\n",
    "# continue if there is any difference between the current and previous assignment\n",
    "is_continue = tf.reduce_any(tf.not_equal(point_to_centroid_assignment, prev_assignments))\n",
    "\n",
    "with tf.control_dependencies([is_continue]):\n",
    "    loop = tf.group(centroids.assign(means), prev_assignments.assign(point_to_centroid_assignment))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# do many iterations\n",
    "has_changed, cnt = True, 0\n",
    "while has_changed and cnt < 300:\n",
    "    cnt += 1\n",
    "    has_changed, _ = sess.run([is_continue, loop])\n",
    "\n",
    "# see how the data is assigned\n",
    "res3 = sess.run(point_to_centroid_assignment)\n",
    "means3 = sess.run(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "points           = tf.Variable(x_train, 'X', dtype=tf.float32)\n",
    "ones_like        = tf.ones((points.get_shape()[0], 1))\n",
    "prev_assignments = tf.Variable(tf.zeros((points.get_shape()[0], ), dtype=tf.int64))\n",
    "\n",
    "# select random points as a starting position. You can do better by randomly selecting k points.\n",
    "start_pos = tf.Variable(x_train[np.random.randint(60000, size=k),:], dtype=tf.float32)\n",
    "centroids = tf.Variable(start_pos.initialized_value(), 'S', dtype=tf.float32)\n",
    "\n",
    "# find the distance between all points: http://stackoverflow.com/a/43839605/1090562\n",
    "p1 = tf.matmul(\n",
    "    tf.expand_dims(tf.reduce_sum(tf.square(points), 1), 1),\n",
    "    tf.ones(shape=(1, k))\n",
    ")\n",
    "p2 = tf.transpose(tf.matmul(\n",
    "    tf.reshape(tf.reduce_sum(tf.square(centroids), 1), shape=[-1, 1]),\n",
    "    ones_like,\n",
    "    transpose_b=True\n",
    "))\n",
    "distance = tf.sqrt(tf.add(p1, p2) - 2 * tf.matmul(points, centroids, transpose_b=True))\n",
    "\n",
    "# assign each point to a closest centroid\n",
    "point_to_centroid_assignment = tf.argmin(distance, axis=1)\n",
    "\n",
    "# recalculate the centers\n",
    "total = tf.unsorted_segment_sum(points, point_to_centroid_assignment, k)\n",
    "count = tf.unsorted_segment_sum(ones_like, point_to_centroid_assignment, k)\n",
    "means = total / count\n",
    "\n",
    "# continue if there is any difference between the current and previous assignment\n",
    "is_continue = tf.reduce_any(tf.not_equal(point_to_centroid_assignment, prev_assignments))\n",
    "\n",
    "with tf.control_dependencies([is_continue]):\n",
    "    loop = tf.group(centroids.assign(means), prev_assignments.assign(point_to_centroid_assignment))\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# do many iterations\n",
    "has_changed, cnt = True, 0\n",
    "while has_changed and cnt < 300:\n",
    "    cnt += 1\n",
    "    has_changed, _ = sess.run([is_continue, loop])\n",
    "\n",
    "# see how the data is assigned\n",
    "res4 = sess.run(point_to_centroid_assignment)\n",
    "means4 = sess.run(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0928 13:12:51.739187 15152 meta_graph.py:449] Issue encountered when serializing variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'X' has type <class 'str'>, but expected one of: (<class 'bool'>, <class 'numbers.Integral'>)\n",
      "W0928 13:12:51.776076 15152 meta_graph.py:449] Issue encountered when serializing trainable_variables.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
      "'X' has type <class 'str'>, but expected one of: (<class 'bool'>, <class 'numbers.Integral'>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outcome: [30]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "a = tf.constant([10])\n",
    "b = tf.constant([20])\n",
    "c = tf.add(a,b)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    writer = tf.summary.FileWriter(\"./log\",sess.graph)\n",
    "    result = sess.run(c)\n",
    "    print(\"outcome:\",result)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Timed out waiting for TensorBoard to start. It may still be running as pid 25636."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
